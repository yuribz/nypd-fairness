{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study of Fairness of Board Decisions in NYPD Complaint Data\n",
    "\n",
    "This project was originally completed for DSC 80 at University of Californoa, San Diego, during Spring 2021 quarter with Professor Justin Eldridge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Findings\n",
    "\n",
    "\n",
    "### Introduction\n",
    "In Project 3, the focus of my work was to determine whether the NYPD complaint data contains any degree of racial bias, in particular researching whether the board would favor the White officers/complaintants in cases against the Black complaitants/officers. My findings suggested that there was a strong indication of racial bias in the dataset, but I lacked some extra information that would help me confirm such a hypothesis with a high degree of certainty. \n",
    "\n",
    "In this project, my task will be to create a prediction model on the same dataset of complaints to the New York Police Department. This time, I am curious to determine whether there is a more general degree of bias that is evident from the dataset by **predicting the board disposition based on the set of features such as ethnicity, age and gender of both the officer and the complainant, as well the allegation against the officer and the reason the complainant was detained in the first place**. This is a classification model, since it will be predicting one of the two possible outcomes (board favored the complainant or not) based on a set of features. The choice of variable reflects the factors that would introduce the degree of bias into the board's decision (age, gender and ethnicity) and also considers the non-biased factors that led to the decision in order to not create a model that is biased in itself and fails to reflect the more important factors that play into the board's disposition.\n",
    "\n",
    "### Baseline Model\n",
    "For my baseline model, I picked a basic set of features that related to the personal information of the complainants and the officers. I had four ordinal features (complainant and officer ages, as well as `fado_type` and `outcome_description`; the first two were passed in as is, while the latter two had to be engineered using two `FunctionTransformers`) and four categorical features (relating to gender and ethnicity, these were encoded using `OneHotEncoder`). My model used `DecisionTreeClassifier` in order to predict `board_disposition` (which was binarized using `FunctionTransformer`, 1 for \"Substantiated\", 0 for \"Not Substantiated\"); `DecisionTreeClassifier` is a reasonable classifier option due to the problem being a classification problem with a set of features that could determine the board outcome. The evaluation metric is the accuracy score, which is reasonable as I am trying to determine how accurate my predictions are compared to the actual outcomes, as I want to see if my model is good at predicting the board disposition given the provided features.\n",
    "\n",
    "The baseline model produced great results on the NYPD dataset, with the accuracy scores being equal to ~0.92 and ~0.72 for training and testing datasets respectively. It seems that the model overfits to the training dataset quite a lot, while also being able to generalize quite well. Still, there is room for improvement, which is the goal of my final model.\n",
    "\n",
    "### Final Model\n",
    "In the final model, I engineered several new features: I binned the ages in order to consolidate age groups and thus decrease fragmentation. The idea behind this is that some age groups might be more favored by the board, therefore I decided to attempt to improve the model by binning the age groups. Another new feature was the binarizing of the `complainant_gender`. In the baseline model, complainant gender is One Hot Encoded because it featured categories outside of Male and Female (it included data on the transgender and non-binary people). I decided to add the binarizing with 1 for Male and 0 for Not Male because of the assumption that the board might be more inclined to substantiate the claims done by non-males rather than males, so I thought that binarization would make the model more accurate. In addition to engineering these two new features, I added the `outcome_description` column to this model, with it being made ordinal with a `FunctionTransformer`.\n",
    "\n",
    "I stuck with a `DecisionTreeClassifier`, and I ran a Grid Search in order to determine the best parameters for the model. The best parameters were calculated as follows:\n",
    "\n",
    "{'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 5}\n",
    "\n",
    "However, after calculating the accuracy score for this model, it turned out that it performed even worse than my baseline model, so I decided to work with the default parameters in order to produce the accuracy scores of ~0.82 and ~0.73 for training and test datasets respectively. The depth of the tree achieved was 51, and the number of leaves was 4260.\n",
    "\n",
    "I was able to improve the model slightly. While the accuracy on the training set dropped significantly, the accuracy on the test set improved by ~0.01. I think this model is a bit better at generalizing to the never-before-seen data, but of course there are still ways to improve the model. I think that me binarizing the gender column and also binning the ages contributed to the improvement of the model.\n",
    "\n",
    "### Fairness Evaluation\n",
    "As an aftermath of Project 3, I am concerned with the racial bias in the dataset. Therefore, I was set out to see if my model is based on the basis of race of the complainants. I binarized the ethnicity data for the complainants (1 for White, 0 for Non White) in order to make the groupings. My metric was using the recall value, as I measured the fairness based on how the proportion of correctly predicted substantiated outcomes for each ethnic group. In other words, I used true positive parity, which is justified by the notion that in a fair model, the board would be as likely to substantiated claims for White and Non-White complainants, so the \"Equality of Opportunity\" parity is reasonable to check whether the model reflects this notion.\n",
    "\n",
    "For the permutation test, I picked 0.01 level of significance due to the relatively small size of the dataset, 500 repeptitions, and the test statistic is the absolute difference in recall values between the groups (of which there are only two); Null Hypothesis stated that there is no difference for White and Non-White complainants, and the Alternative Hypothesis suggested that there is a difference and the model is biased. The observed difference was ~0.021, and the p-value produced by the permutation test was 0.706, meaning that I fail to reject my Null Hypothesis. I cannot confirm that there is no difference in recall values for the two groups and that there is no bias involved, but considering the mathematical computations of sklearn, I have reasons to believe that my model might be fair, despite my findings in Project 3 that suggested a degree of racial bias.\n",
    "\n",
    "### Conclusion\n",
    "This was an interesting project to explore, and a great follow up to the analysis in Project 3. I definitely could improve my final model even further, but I was greatly limited with the amount of features I could engineer from the provided dataset. Still, I believe I was able to fine tune my model to an acceptable degree, and going through a process of preparing, training and evaluating a model and its fairness is a great experience that will be helpful in my future career in Data Science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn import pipeline\n",
    "from sklearn import compose\n",
    "import re\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  # Higher resolution figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I need to load in the NYPD complaints dataset, which I have in the same directory. I will also make a copy of the DataFrame with the original dataset to have it as a reference, in case `nypd` DataFrames gets overwritten (which does happen later in the code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "nypd = pd.read_csv(\"nypd_data.csv\")\n",
    "original_nypd = nypd.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what the DataFrame looks like -- `original_nypd` looks exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_mos_id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>command_now</th>\n",
       "      <th>shield_no</th>\n",
       "      <th>complaint_id</th>\n",
       "      <th>month_received</th>\n",
       "      <th>year_received</th>\n",
       "      <th>month_closed</th>\n",
       "      <th>year_closed</th>\n",
       "      <th>...</th>\n",
       "      <th>mos_age_incident</th>\n",
       "      <th>complainant_ethnicity</th>\n",
       "      <th>complainant_gender</th>\n",
       "      <th>complainant_age_incident</th>\n",
       "      <th>fado_type</th>\n",
       "      <th>allegation</th>\n",
       "      <th>precinct</th>\n",
       "      <th>contact_reason</th>\n",
       "      <th>outcome_description</th>\n",
       "      <th>board_disposition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10004</td>\n",
       "      <td>Jonathan</td>\n",
       "      <td>Ruiz</td>\n",
       "      <td>078 PCT</td>\n",
       "      <td>8409</td>\n",
       "      <td>42835</td>\n",
       "      <td>7</td>\n",
       "      <td>2019</td>\n",
       "      <td>5</td>\n",
       "      <td>2020</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>Abuse of Authority</td>\n",
       "      <td>Failure to provide RTKA card</td>\n",
       "      <td>78.0</td>\n",
       "      <td>Report-domestic dispute</td>\n",
       "      <td>No arrest made or summons issued</td>\n",
       "      <td>Substantiated (Command Lvl Instructions)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10007</td>\n",
       "      <td>John</td>\n",
       "      <td>Sears</td>\n",
       "      <td>078 PCT</td>\n",
       "      <td>5952</td>\n",
       "      <td>24601</td>\n",
       "      <td>11</td>\n",
       "      <td>2011</td>\n",
       "      <td>8</td>\n",
       "      <td>2012</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>Discourtesy</td>\n",
       "      <td>Action</td>\n",
       "      <td>67.0</td>\n",
       "      <td>Moving violation</td>\n",
       "      <td>Moving violation summons issued</td>\n",
       "      <td>Substantiated (Charges)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10007</td>\n",
       "      <td>John</td>\n",
       "      <td>Sears</td>\n",
       "      <td>078 PCT</td>\n",
       "      <td>5952</td>\n",
       "      <td>24601</td>\n",
       "      <td>11</td>\n",
       "      <td>2011</td>\n",
       "      <td>8</td>\n",
       "      <td>2012</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>Offensive Language</td>\n",
       "      <td>Race</td>\n",
       "      <td>67.0</td>\n",
       "      <td>Moving violation</td>\n",
       "      <td>Moving violation summons issued</td>\n",
       "      <td>Substantiated (Charges)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10007</td>\n",
       "      <td>John</td>\n",
       "      <td>Sears</td>\n",
       "      <td>078 PCT</td>\n",
       "      <td>5952</td>\n",
       "      <td>26146</td>\n",
       "      <td>7</td>\n",
       "      <td>2012</td>\n",
       "      <td>9</td>\n",
       "      <td>2013</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>45.0</td>\n",
       "      <td>Abuse of Authority</td>\n",
       "      <td>Question</td>\n",
       "      <td>67.0</td>\n",
       "      <td>PD suspected C/V of violation/crime - street</td>\n",
       "      <td>No arrest made or summons issued</td>\n",
       "      <td>Substantiated (Charges)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10009</td>\n",
       "      <td>Noemi</td>\n",
       "      <td>Sierra</td>\n",
       "      <td>078 PCT</td>\n",
       "      <td>24058</td>\n",
       "      <td>40253</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "      <td>...</td>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Force</td>\n",
       "      <td>Physical force</td>\n",
       "      <td>67.0</td>\n",
       "      <td>Report-dispute</td>\n",
       "      <td>Arrest - other violation/crime</td>\n",
       "      <td>Substantiated (Command Discipline A)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_mos_id first_name last_name command_now  shield_no  complaint_id  \\\n",
       "0          10004   Jonathan      Ruiz     078 PCT       8409         42835   \n",
       "1          10007       John     Sears     078 PCT       5952         24601   \n",
       "2          10007       John     Sears     078 PCT       5952         24601   \n",
       "3          10007       John     Sears     078 PCT       5952         26146   \n",
       "4          10009      Noemi    Sierra     078 PCT      24058         40253   \n",
       "\n",
       "   month_received  year_received  month_closed  year_closed  ...  \\\n",
       "0               7           2019             5         2020  ...   \n",
       "1              11           2011             8         2012  ...   \n",
       "2              11           2011             8         2012  ...   \n",
       "3               7           2012             9         2013  ...   \n",
       "4               8           2018             2         2019  ...   \n",
       "\n",
       "  mos_age_incident complainant_ethnicity complainant_gender  \\\n",
       "0               32                 Black             Female   \n",
       "1               24                 Black               Male   \n",
       "2               24                 Black               Male   \n",
       "3               25                 Black               Male   \n",
       "4               39                   NaN                NaN   \n",
       "\n",
       "  complainant_age_incident           fado_type                    allegation  \\\n",
       "0                     38.0  Abuse of Authority  Failure to provide RTKA card   \n",
       "1                     26.0         Discourtesy                        Action   \n",
       "2                     26.0  Offensive Language                          Race   \n",
       "3                     45.0  Abuse of Authority                      Question   \n",
       "4                     16.0               Force                Physical force   \n",
       "\n",
       "  precinct                                contact_reason  \\\n",
       "0     78.0                       Report-domestic dispute   \n",
       "1     67.0                              Moving violation   \n",
       "2     67.0                              Moving violation   \n",
       "3     67.0  PD suspected C/V of violation/crime - street   \n",
       "4     67.0                                Report-dispute   \n",
       "\n",
       "                outcome_description                         board_disposition  \n",
       "0  No arrest made or summons issued  Substantiated (Command Lvl Instructions)  \n",
       "1   Moving violation summons issued                   Substantiated (Charges)  \n",
       "2   Moving violation summons issued                   Substantiated (Charges)  \n",
       "3  No arrest made or summons issued                   Substantiated (Charges)  \n",
       "4    Arrest - other violation/crime      Substantiated (Command Discipline A)  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nypd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the list of all the columns that are in the DataFrame. For my baseline model, I want to look at the basic info that might affect the board decisions: gender of both the officer and the complainant, their ethnicity, age, the type of complaint, the result of the interaction, etc. I listed these columns in the next cell, storing them in the `req_columns` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unique_mos_id', 'first_name', 'last_name', 'command_now', 'shield_no',\n",
       "       'complaint_id', 'month_received', 'year_received', 'month_closed',\n",
       "       'year_closed', 'command_at_incident', 'rank_abbrev_incident',\n",
       "       'rank_abbrev_now', 'rank_now', 'rank_incident', 'mos_ethnicity',\n",
       "       'mos_gender', 'mos_age_incident', 'complainant_ethnicity',\n",
       "       'complainant_gender', 'complainant_age_incident', 'fado_type',\n",
       "       'allegation', 'precinct', 'contact_reason', 'outcome_description',\n",
       "       'board_disposition'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nypd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_columns = [\n",
    "        'mos_ethnicity', 'mos_gender', 'mos_age_incident',\n",
    "        'complainant_ethnicity', 'complainant_gender',\n",
    "        'complainant_age_incident', 'fado_type', \n",
    "        'outcome_description', 'board_disposition'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing the DataFrame to only get the required columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mos_ethnicity</th>\n",
       "      <th>mos_gender</th>\n",
       "      <th>mos_age_incident</th>\n",
       "      <th>complainant_ethnicity</th>\n",
       "      <th>complainant_gender</th>\n",
       "      <th>complainant_age_incident</th>\n",
       "      <th>fado_type</th>\n",
       "      <th>outcome_description</th>\n",
       "      <th>board_disposition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hispanic</td>\n",
       "      <td>M</td>\n",
       "      <td>32</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>Abuse of Authority</td>\n",
       "      <td>No arrest made or summons issued</td>\n",
       "      <td>Substantiated (Command Lvl Instructions)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>White</td>\n",
       "      <td>M</td>\n",
       "      <td>24</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>Discourtesy</td>\n",
       "      <td>Moving violation summons issued</td>\n",
       "      <td>Substantiated (Charges)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>White</td>\n",
       "      <td>M</td>\n",
       "      <td>24</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>Offensive Language</td>\n",
       "      <td>Moving violation summons issued</td>\n",
       "      <td>Substantiated (Charges)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>White</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>45.0</td>\n",
       "      <td>Abuse of Authority</td>\n",
       "      <td>No arrest made or summons issued</td>\n",
       "      <td>Substantiated (Charges)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hispanic</td>\n",
       "      <td>F</td>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Force</td>\n",
       "      <td>Arrest - other violation/crime</td>\n",
       "      <td>Substantiated (Command Discipline A)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mos_ethnicity mos_gender  mos_age_incident complainant_ethnicity  \\\n",
       "0      Hispanic          M                32                 Black   \n",
       "1         White          M                24                 Black   \n",
       "2         White          M                24                 Black   \n",
       "3         White          M                25                 Black   \n",
       "4      Hispanic          F                39                   NaN   \n",
       "\n",
       "  complainant_gender  complainant_age_incident           fado_type  \\\n",
       "0             Female                      38.0  Abuse of Authority   \n",
       "1               Male                      26.0         Discourtesy   \n",
       "2               Male                      26.0  Offensive Language   \n",
       "3               Male                      45.0  Abuse of Authority   \n",
       "4                NaN                      16.0               Force   \n",
       "\n",
       "                outcome_description                         board_disposition  \n",
       "0  No arrest made or summons issued  Substantiated (Command Lvl Instructions)  \n",
       "1   Moving violation summons issued                   Substantiated (Charges)  \n",
       "2   Moving violation summons issued                   Substantiated (Charges)  \n",
       "3  No arrest made or summons issued                   Substantiated (Charges)  \n",
       "4    Arrest - other violation/crime      Substantiated (Command Discipline A)  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nypd = nypd[req_columns]\n",
    "nypd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the columns that I have gotten, I will be making features for my baseline model. There will be eight features engineered, one per column, out of which:\n",
    "* Two are ordinal and correspond to ages. They are not modified in any way (for now).\n",
    "* One more is also ordinal, and corresponds to `fado_type`. It will be transformed in order to convert the string to categories to numeric values based on the type: discourtesy (lowest), offensive language, abuse of authority and use of force (the largest).\n",
    "* Another ordinal feature based on `outcome_description`: 0 for no outcome, 1 for summons and juvenile reports, 2 for arrests.\n",
    "* Categorical features related to gender and ethnicity, which will be transformed using One Hot Encoding (gender is binary for officers, but has more than two categories for complainants)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I needed to set up a function that would create the ordinal encoding for `fado_type`. The next several cells fetch the info about the column by exploring the unique entries and defines a function that would process a column containing these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Abuse of Authority', 'Discourtesy', 'Offensive Language', 'Force'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nypd['fado_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "fado_types = [\n",
    "    'Discourtesy',\n",
    "    'Offensive Language',\n",
    "    'Abuse of Authority',\n",
    "    'Force'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_board(disp):\n",
    "    if isinstance(disp, pd.DataFrame):\n",
    "        return pd.DataFrame(pd.Series(disp.values[:,0]).apply(transform_board))\n",
    "    return int(\"Substantiated\" in disp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar process is done for `outcome_description`, which will also require grouping into types of outcome: arrest, summons/report, or neither. The function is defined here and will perform the needed type of grouping and encoding, and then will be passed to the function transformer later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['No arrest made or summons issued',\n",
       "       'Moving violation summons issued',\n",
       "       'Arrest - other violation/crime',\n",
       "       'Summons - other violation/crime', 'Arrest - OGA',\n",
       "       'Other VTL violation summons issued', 'Arrest - resisting arrest',\n",
       "       'Arrest - disorderly conduct', 'Arrest - assault (against a PO)',\n",
       "       'Summons - disorderly conduct', 'Juvenile Report',\n",
       "       'Parking summons issued', 'Disorderly-Conduct/Arr/Summons',\n",
       "       'Assault/Arrested', 'Other Summons Claimed or Issued', nan,\n",
       "       'Arrest - harrassment (against a PO)', 'Arrest on Other Charge',\n",
       "       'Obstruct-Govt-Admin/Arrested',\n",
       "       'Traffic Summons Claimed or Issued', 'Resisting Arrest/Arrested',\n",
       "       'Harrassment/Arrested/Summons', 'Summons - OGA',\n",
       "       'Summons - harrassment (against a PO)'], dtype=object)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nypd['outcome_description'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_outcome(out):\n",
    "    if isinstance(out, pd.DataFrame):\n",
    "        return pd.DataFrame(pd.Series(out.values[:,0]).apply(transform_outcome))\n",
    "    if ('No arrest') in out:\n",
    "        return 0\n",
    "    elif ('Arrest') in out:\n",
    "        return 2\n",
    "    elif (('Summons') in out) or ('Juvenile' in out) or (('summons') in out and not (('arrest') in out)):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell looks at the values for Board Disposition: these would require grouping into \"Favor the complainant\" and \"Favor the officer\" values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Substantiated (Command Lvl Instructions)\n",
       "1                         Substantiated (Charges)\n",
       "2                         Substantiated (Charges)\n",
       "3                         Substantiated (Charges)\n",
       "4            Substantiated (Command Discipline A)\n",
       "                           ...                   \n",
       "33353                             Unsubstantiated\n",
       "33354                             Unsubstantiated\n",
       "33355         Substantiated (Formalized Training)\n",
       "33356         Substantiated (Formalized Training)\n",
       "33357         Substantiated (Formalized Training)\n",
       "Length: 33358, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(nypd[['board_disposition']].values[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell stores the names of the columns that would undergo OneHotEncoding: these are gender and ethnicity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_columns = ['mos_gender', 'complainant_gender', 'mos_ethnicity', 'complainant_ethnicity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the functions defined, I created a `transformer`, which is an sklearn ColumnTransformer. It will be performing the preprocessing for the pipeline of my baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = sklearn.preprocessing.OneHotEncoder(handle_unknown = 'ignore')\n",
    "\n",
    "ordinal_fado = sklearn.preprocessing.OrdinalEncoder(categories = [fado_types])\n",
    "\n",
    "# the board transformer is defined here, but will come in later\n",
    "board = sklearn.preprocessing.FunctionTransformer(transform_board)\n",
    "\n",
    "outcome = sklearn.preprocessing.FunctionTransformer(transform_outcome)\n",
    "\n",
    "transformer = sklearn.compose.ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('ohe', ohe, ohe_columns),\n",
    "        ('fado', ordinal_fado, ['fado_type']),\n",
    "        ('outcome', outcome, ['outcome_description']),\n",
    "    ], remainder = \"passthrough\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell creates a copy of the dataset that doesn't contain any `NaN` values (as they would break the model). The choice of dropping the missing values is reasonable, since of most of the data missing from the dataset is complainant's gender, age or ethnicity. These are some of the key features in my model, and filling them with arbitrary values could make the model biased. A reasonable approach would be to perform data imputation, however for the sake of simplicity for this baseline model, I decided to perform classification without the missing values altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mos_ethnicity                  0\n",
       "mos_gender                     0\n",
       "mos_age_incident               0\n",
       "complainant_ethnicity       4464\n",
       "complainant_gender          4195\n",
       "complainant_age_incident    4812\n",
       "fado_type                      0\n",
       "outcome_description           56\n",
       "board_disposition              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this cell demonstrates which columns have the most missing values -- \n",
    "# indeed, it's the personal information of the complainants\n",
    "nypd.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_nypd = nypd.dropna()[req_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the transformer and transforming the `clean_nypd` dataset in order to ensure that the preprocesor works as intended. It looks like the result is what I need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(remainder='passthrough',\n",
       "                  transformers=[('ohe', OneHotEncoder(handle_unknown='ignore'),\n",
       "                                 ['mos_gender', 'complainant_gender',\n",
       "                                  'mos_ethnicity', 'complainant_ethnicity']),\n",
       "                                ('fado',\n",
       "                                 OrdinalEncoder(categories=[['Discourtesy',\n",
       "                                                             'Offensive '\n",
       "                                                             'Language',\n",
       "                                                             'Abuse of '\n",
       "                                                             'Authority',\n",
       "                                                             'Force']]),\n",
       "                                 ['fado_type']),\n",
       "                                ('outcome',\n",
       "                                 FunctionTransformer(func=<function transform_outcome at 0x000001A468A238B0>),\n",
       "                                 ['outcome_description'])])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.fit(clean_nypd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 1.0, 1.0, ..., 32, 38.0,\n",
       "        'Substantiated (Command Lvl Instructions)'],\n",
       "       [0.0, 1.0, 0.0, ..., 24, 26.0, 'Substantiated (Charges)'],\n",
       "       [0.0, 1.0, 0.0, ..., 24, 26.0, 'Substantiated (Charges)'],\n",
       "       ...,\n",
       "       [0.0, 1.0, 0.0, ..., 36, 21.0,\n",
       "        'Substantiated (Formalized Training)'],\n",
       "       [0.0, 1.0, 0.0, ..., 36, 21.0,\n",
       "        'Substantiated (Formalized Training)'],\n",
       "       [0.0, 1.0, 0.0, ..., 36, 21.0,\n",
       "        'Substantiated (Formalized Training)']], dtype=object)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(transformer.transform(clean_nypd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to create the Pipeline. For my model, I decided to use the DecisionTreeClassifier, since my model is a classification model with two options (board favors the complaint or not). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pl = sklearn.pipeline.Pipeline(\n",
    "    steps = [\n",
    "        ('preprocessor', transformer),\n",
    "        ('classifier', DecisionTreeClassifier())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell splits the dataset into the training subset and the test subset. Aditionally, it transforms the `board_disposition` column, which will be the target (`y`) that the model will be trying to predict. This is where the `board` FunctionTransformer is used that was defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = clean_nypd.drop('board_disposition', axis = 1)\n",
    "# y = transform_board(clean_nypd[['board_disposition']])\n",
    "y = board.fit_transform(clean_nypd[['board_disposition']])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the data, and then predicting the values. The cell will be outputing the accuracy score that evaluates how close my predictions are. On the training set, the model predicts expected 0.91 (pretty close), while on the training set the accuracy is somewhere around 0.72: this is pretty good, but could be better.\n",
    "\n",
    "I stored the accuracy scores I got from this baseline model in order to compare them to the improved model that I created later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9195679977263038, 0.7212276214833759)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.fit(X_train, y_train)\n",
    "original_model_train = pl.score(X_train, y_train)\n",
    "original_model_test = pl.score(X_test, y_test)\n",
    "original_model_train, original_model_test\n",
    "# the left value is on the training set, the right one is on the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model could be improved. To see how I could make it better, I decided to pull up the list of all the columns in hopes of finding some that might make my model even better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unique_mos_id', 'first_name', 'last_name', 'command_now', 'shield_no',\n",
       "       'complaint_id', 'month_received', 'year_received', 'month_closed',\n",
       "       'year_closed', 'command_at_incident', 'rank_abbrev_incident',\n",
       "       'rank_abbrev_now', 'rank_now', 'rank_incident', 'mos_ethnicity',\n",
       "       'mos_gender', 'mos_age_incident', 'complainant_ethnicity',\n",
       "       'complainant_gender', 'complainant_age_incident', 'fado_type',\n",
       "       'allegation', 'precinct', 'contact_reason', 'outcome_description',\n",
       "       'board_disposition'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_nypd.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have some ideas on how I can improve my model:\n",
    "* The age is a bit too fragmented in its current form -- it would make more sense to try to predict the board disposition using age bins, rather than try to calculate it year by year. For example, teens might be more likely to be favored by the board compared to adults, same with senior citizens. Therefore, binning makes total sense and might yield more accurate results.\n",
    "* The gender column is way too fragmented for the complainants, with values specifying whether the complainant is transgender or non-binary. For officers, the data simply states whether they are male or female. To make the model more precise and to make it more unified, I decided to binarize the gender feature: now it is simply checking whether the person, both officer and the complainant, are male or not (cis and transgender men are grouped together). The issue with this grouping is that non-binary people are grouped with women, which has the potential of introducing bias. However, the board might base the their decision on gender, and men historically have been more likely to get harsher treatment by law enforcement, so I believe it is reasonable to train the model to check whether the person is male or not. For now, in efforts to improve the model, I will create this binarization and will evaluate the resulting fairness later.\n",
    "* I added the `contact_reason` column, which specifies the reason for the officer/complainant contact (i.e. the crime that the complainant allegedly commited). This, in my opinion, will be main the feature that will lead to major improvement of my model, since it will contain information about why the complainant interacted with the officer in the first place, and harsher crimes might lead to the board not favoring the complaint.\n",
    "\n",
    "I expect that these new features (binned age and binarized gender) will improve my model by decreasing fragmentation and avoiding overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell creates several transformers that will engineer the new features that I outlined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_bins = sklearn.preprocessing.KBinsDiscretizer(strategy = 'uniform', encode = 'ordinal')\n",
    "ohe_columns.append('contact_reason')\n",
    "\n",
    "def is_male(x):\n",
    "    return pd.DataFrame(\n",
    "        pd.Series(x.values[:,0])\n",
    "        .str.contains('(?:M|Male|FTM)$', regex=True)\n",
    "        .astype('int')\n",
    "    )\n",
    "\n",
    "is_male = sklearn.preprocessing.FunctionTransformer(is_male)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessor for the final model, using some of the features from the baseline model and including the newly engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_transformer = sklearn.compose.ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('ohe', ohe, ohe_columns),\n",
    "        ('fado', ordinal_fado, ['fado_type']),\n",
    "        ('outcome', outcome, ['outcome_description']),\n",
    "        ('male', is_male, ['mos_gender', 'complainant_gender']),\n",
    "        ('age_bins', age_bins, ['mos_age_incident', 'complainant_age_incident'])\n",
    "    ], remainder = \"passthrough\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make my model more precise and to generalize it better, I also decided to perform the GridSearchCV in order to determine the best parameters for the DecisionTreeClassifier. It uses five cross validation folds and tests four different options for `min_samples_leaf` and `min_samples_split`, and six for `max_depth`. After GridSearchCV runs, I will use these parameters in my final pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'contact_reason' not in req_columns:\n",
    "    req_columns.append('contact_reason')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5, 'min_samples_leaf': 10, 'min_samples_split': 5}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    'max_depth': [5, 8, 10, 12, 15, None], \n",
    "    'min_samples_split':[5, 10, 15, 20],\n",
    "    'min_samples_leaf':[5, 10, 15, 20]\n",
    "}\n",
    "\n",
    "X_grid = pd.DataFrame(final_transformer.fit_transform(clean_nypd.drop('board_disposition', axis = 1)).toarray())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_grid, board.fit_transform(clean_nypd[['board_disposition']]))\n",
    "\n",
    "clf = GridSearchCV(DecisionTreeClassifier(), parameters, cv = 5)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines the pipeline for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pl = sklearn.pipeline.Pipeline(\n",
    "    steps = [\n",
    "        ('preprocessing', final_transformer),\n",
    "        ('classifier', DecisionTreeClassifier(\n",
    "            max_depth = 5,\n",
    "            min_samples_leaf= 10,\n",
    "            min_samples_split= 5\n",
    "        ))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to fit and run the model, storing the accuracy scores for both the training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_nypd = clean_nypd.drop('contact_reason', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7546776561981905, 0.751207729468599)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_nypd = original_nypd[req_columns].dropna()\n",
    "X = clean_nypd.drop('board_disposition', axis = 1)\n",
    "y = transform_board(clean_nypd[['board_disposition']])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "final_pl.fit(X_train, y_train)\n",
    "final_pl.score(X_train, y_train), final_pl.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like that this model turned out to be even worse than my original model. After some testing, I concluded this is due to the parameters that I used in my DecisionTreeClassifier (the ones determined by the grid search). Therefore, for my final model, I will be sticking with the 'as-is' parameters, as they seem to work just fine on this dataset. I will be keeping the newly engineered features, however, as I still believe that binned ages and binarized gender should improve my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8236464402444224, 0.7328786587098608)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pl = sklearn.pipeline.Pipeline(\n",
    "    steps = [\n",
    "        ('preprocessing', final_transformer),\n",
    "        ('classifier', DecisionTreeClassifier())\n",
    "    ]\n",
    ")\n",
    "\n",
    "clean_nypd = original_nypd[req_columns].dropna()\n",
    "X = clean_nypd.drop('board_disposition', axis = 1)\n",
    "y = transform_board(clean_nypd[['board_disposition']])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "final_pl.fit(X_train, y_train)\n",
    "final_pl.score(X_train, y_train), final_pl.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9195679977263038, 0.7212276214833759)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline model accuracy scores, for reference\n",
    "original_model_train, original_model_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better. The model is still behaving less accurately on the training dataset, but it seems to be doing slightly better on the test dataset, suggesting a slight improvement in terms of generalization. This is still far from a perfect model, but at least I am able to see some improvement: the final model produces the accuracy score that is higher by 0.01, which is noticeable. For now, I am satistifed with how I was able to improve my model by engineering new featuers (binned ages and binarized genders) and determining which DecisionTreeClassifier parameters work best (as it turns out, the default ones work the best). Now, I will move on to assess the fairness of my model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the parameters that my model used for this evaluation:\n",
    "\n",
    "* Depth = 51\n",
    "* Number of leaves = 4260"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 4260)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pl.named_steps['classifier'].get_depth(), final_pl.named_steps['classifier'].get_n_leaves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My original analysis in Project 3 focused on the racial bias in the NYPD data. Now, I am curious to find out whether the model that I build carries out unfair predictions based on the race. I will use a binary system in order to determine that: I will group the ethnicities of the complainants into two groups, White and Non-White. The reasoning behind that is because, as I found out in Project 3, White complainants demonstrated to be favored by the board when submitting claims against Black officers. Now, the idea is to see whether the model is more likely to predict the substantiated board disposition for White complaintants compared to complaints submitted by ethnic minorities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do my evaluation, I would first need to fetch the predictions from my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = final_pl.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using `sklearn.metrics` in order to evaluate the **recall** of my data. The reason I am using recall is because I want to see the proportion of true positives for each ethnic category (White or Non-White), as positives correspond to the favorable board disposition. So, by comparing the correct predictions, I will be able to see which group is more likely to be correctly predicted to be favorably prefered by the board. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20207,  1005],\n",
       "       [ 4071,  2866]], dtype=int64)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "metrics.confusion_matrix(y[0].values, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might pose a problem: it seems that my predictions mostly predicted negative outcomes (only 14% of the predictions are favorable). This might be the direct outcome of how I grouped board dispositions: in the original dataset, Substantiated/Unsubstantiated/Exonerated each had around a third of the dataset, while in my model I grouped the latter two together, creating a 33/66 split. This is something I need to consider in my evaluation, and I believe that it shouldn't pose a problem since the difference in the recall should still be evident if there is unfairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1375182066858503"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will collect the counts of True Negatives, False Positives, False Negatives and True Positives from the confusion matrix in order to calculate the recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = metrics.confusion_matrix(y.values, preds).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4131468934697996"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall = tp / (tp + fn)\n",
    "recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the recall for the entire dataset. As it seems, it is at 0.413, which is average for the entire dataset. What I will do is I will assign the prediction column and the actual categories (represented by `y`) to the `complainant_ethnciity` column, engineered in a way that only includes the info on whether the complainant is White or not. Then, I will perform the permutation test and will use a test statistic to determine whether there is a discrepancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "As my test statistic, I want to find the difference between the recall values for each of the subsets. So, as a test statistic I will use **absolute distance**. This is a reasonable metric because all I need to do is determine how close are two values for each of the subsets. I will pick a significance level of 0.01 (this is a small dataset, so I am more likely to observe extreme values) and I will run the test 500 times. \n",
    "\n",
    "**Null Hypothesis**: White and Non-White groups have similar recall values; the model is fair.\n",
    "\n",
    "**Alternative Hypothesis**: The two groups have noticeably different recall values, suggesting bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell creates a transformer that will be binarizing the ethnicity column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_white = sklearn.preprocessing.FunctionTransformer(\n",
    "    lambda x: pd.DataFrame(\n",
    "        pd.Series(x.values[:,0]).str.contains('White').astype('int')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These cells build a new DataFrame that I will be using in my evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complainant_ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  complainant_ethnicity\n",
       "0                 Black\n",
       "1                 Black\n",
       "2                 Black\n",
       "3                 Black\n",
       "4                 White"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fairness_df = original_nypd[['complainant_ethnicity']].dropna().reset_index(drop=True)\n",
    "fairness_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complainant_ethnicity</th>\n",
       "      <th>predictions</th>\n",
       "      <th>actual_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   complainant_ethnicity  predictions  actual_values\n",
       "0                      0          1.0            1.0\n",
       "1                      0          0.0            1.0\n",
       "2                      0          1.0            1.0\n",
       "3                      0          0.0            1.0\n",
       "4                      1          1.0            1.0"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fairness_df = fairness_df.assign(**{\n",
    "    'complainant_ethnicity' : is_white.fit_transform(fairness_df),\n",
    "    'predictions' : pd.Series(preds),\n",
    "    'actual_values' : y\n",
    "})\n",
    "fairness_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the values turned out to be missing. I will simply drop them because it seems that the values that are missing are the predictions and the actual values. This is most likely caused by the fact that when I ran the model, I used `dropna` which dropped ethnicity information in addition to gender and age, so it is likely that the shape of the prediction array is different from the ethnicity information. I don't think it will affect my analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "complainant_ethnicity      0\n",
       "predictions              745\n",
       "actual_values            745\n",
       "dtype: int64"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fairness_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "complainant_ethnicity    0\n",
       "predictions              0\n",
       "actual_values            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fairness_df = fairness_df.dropna()\n",
    "fairness_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates recall given a DataFrame of the format similar to `fairness_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_recall(X):\n",
    "    preds_arr = X['predictions'].values\n",
    "    vals_arr = X['actual_values'].values\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(vals_arr, preds_arr).ravel()\n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4131468934697996"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_recall(fairness_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will define a function `find_recall_diff` that will calculate the difference between recalls across two groups. The result that is outputed by me calling the function on the `fairness_df` is my **observed test statistic**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_recall_diff(X):\n",
    "    slices = X['complainant_ethnicity'] == 1\n",
    "    white = X[slices]\n",
    "    non_white = X[~slices]\n",
    "    return np.abs(find_recall(white) - find_recall(non_white))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020524866712295697"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed = find_recall_diff(fairness_df)\n",
    "observed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for the permutation test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.706"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(500):\n",
    "    shuffled = fairness_df.assign(**{\n",
    "        'complainant_ethnicity': fairness_df.sample(replace=False, frac=1).reset_index(drop=True)\n",
    "    })\n",
    "    \n",
    "    results.append(find_recall_diff(shuffled))\n",
    "\n",
    "p_val = np.mean(observed >= np.array(results))\n",
    "p_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My test produced a p-value of 0.706, which is not statistically significant. This means that I fail to reject my Null Hypothesis. This is not something that I expected, as I expected a certain degree of racial bias to show up especially after the findings in Project 3. However, perhaps the usage of different metrics and approaches allowed for a more comprehensive look into the dataset. The sklearn model performed mathematical calculations that weighted all the features and made the best use of them, so I have reasons to assume that the model reflects the actual dataset quite well. Threfore, based on that, I can assume that the model is not biased, and, by extension, I have the reason to believe that the dataset of my features also doesn't demonstrate a degree of racial bias, at least with the features engineered for my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
